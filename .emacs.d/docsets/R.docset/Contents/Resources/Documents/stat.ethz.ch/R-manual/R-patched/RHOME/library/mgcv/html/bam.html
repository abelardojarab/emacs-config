<html><!-- Mirrored from stat.ethz.ch/R-manual/R-patched/RHOME/library/mgcv/html/bam.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 04 May 2016 17:01:38 GMT --><!-- Added by HTTrack --><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><!-- /Added by HTTrack -->
<title>Generalized additive models for very large datasets</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">
</head><body>

<table width="100%" summary="page for bam {mgcv}"><tbody><tr><td>bam {mgcv}</td><td style="text-align: right;">R Documentation</td></tr></tbody></table>

<h2>Generalized additive models for very large datasets</h2>

<h3>Description</h3>

<p> Fits a generalized additive model (GAM) to a very large
data set, the term ‘GAM’ being taken to include any quadratically penalized GLM.  
The degree of smoothness of model terms is estimated as part of
fitting. In use the function is much like <code><a href="gam.html">gam</a></code>, except that the numerical methods
are designed for datasets containing upwards of several tens of thousands of data (see Wood, Goude and Shaw, 2015). The advantage 
of <code>bam</code> is much lower memory footprint than <code><a href="gam.html">gam</a></code>, but it can also be much faster, 
for large datasets. <code>bam</code> can also compute on a cluster set up by the parallel package.
</p>
<p>An alternative fitting approach is provided by the <code>discrete==TRUE</code> method. In this case a method based on discretization of 
covariate values and C code level parallelization (controlled by the <code>nthreads</code> argument instead of the <code>cluster</code> argument) is used. This extends both the data set and model size usable.
</p>


<h3>Usage</h3>

<pre>bam(formula,family=gaussian(),data=list(),weights=NULL,subset=NULL,
    na.action=na.omit, offset=NULL,method="fREML",control=list(),
    select=FALSE,scale=0,gamma=1,knots=NULL,sp=NULL,min.sp=NULL,
    paraPen=NULL,chunk.size=10000,rho=0,AR.start=NULL,discrete=FALSE,
    sparse=FALSE,cluster=NULL,nthreads=NA,gc.level=1,use.chol=FALSE,
    samfrac=1,drop.unused.levels=TRUE,G=NULL,fit=TRUE,...)
</pre>


<h3>Arguments</h3>

 
<table summary="R argblock">
<tbody><tr valign="top"><td><code>formula</code></td>
<td>
<p> A GAM formula (see <code><a href="formula.gam.html">formula.gam</a></code> and also <code><a href="gam.models.html">gam.models</a></code>). 
This is exactly like the formula for a GLM except that smooth terms, <code>s</code> and <code>te</code> can be added 
to the right hand side to specify that the linear predictor depends on smooth functions of predictors 
(or linear functionals of these).
</p>
</td></tr> 
<tr valign="top"><td><code>family</code></td>
<td>

<p>This is a family object specifying the distribution and link to use in
fitting etc. See <code><a href="../../stats/html/glm.html">glm</a></code> and <code><a href="../../stats/html/family.html">family</a></code> for more
details. A negative binomial family is provided: see <code><a href="negbin.html">negbin</a></code>, but
only the known theta case is supported by <code>bam</code>.
</p>
</td></tr> 
<tr valign="top"><td><code>data</code></td>
<td>
<p> A data frame or list containing the model response variable and 
covariates required by the formula. By default the variables are taken 
from <code>environment(formula)</code>: typically the environment from 
which <code>gam</code> is called.</p>
</td></tr> 
<tr valign="top"><td><code>weights</code></td>
<td>
<p>  prior weights on the contribution of the data to the log likelihood. Note that a weight of 2, for example, 
is equivalent to having made exactly the same observation twice. If you want to reweight the contributions 
of each datum without changing the overall magnitude of the log likelihood, then you should normalize the weights
(e.g. <code>weights &lt;- weights/mean(weights)</code>).</p>
</td></tr>
<tr valign="top"><td><code>subset</code></td>
<td>
<p> an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr valign="top"><td><code>na.action</code></td>
<td>
<p> a function which indicates what should happen when the data
contain ‘NA’s.  The default is set by the ‘na.action’ setting
of ‘options’, and is ‘na.fail’ if that is unset.  The
“factory-fresh” default is ‘na.omit’.</p>
</td></tr>
<tr valign="top"><td><code>offset</code></td>
<td>
<p>Can be used to supply a model offset for use in fitting. Note
that this offset will always be completely ignored when predicting, unlike an offset 
included in <code>formula</code> (this used to conform to the behaviour of
<code>lm</code> and <code>glm</code>).</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>The smoothing parameter estimation method. <code>"GCV.Cp"</code> to use GCV for unknown scale parameter and
Mallows' Cp/UBRE/AIC for known scale. <code>"GACV.Cp"</code> is equivalent, but using GACV in place of GCV. <code>"REML"</code> 
for REML estimation, including of unknown scale, <code>"P-REML"</code> for REML estimation, but using a Pearson estimate 
of the scale. <code>"ML"</code> and <code>"P-ML"</code> are similar, but using maximum likelihood in place of REML. Default 
<code>"fREML"</code> uses fast REML computation.</p>
</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>
<p>A list of fit control parameters to replace defaults returned by 
<code><a href="gam.control.html">gam.control</a></code>. Any control parameters not supplied stay at their default values.</p>
</td></tr>
<tr valign="top"><td><code>select</code></td>
<td>
<p>Should selection penalties be added to the smooth effects, so that they can in principle be 
penalized out of the model? Has the side effect that smooths no longer have a fixed effect component (improper prior
from a Bayesian perspective) allowing REML comparison of models with the same fixed effect structure. 
</p>
</td></tr>
<tr valign="top"><td><code>scale</code></td>
<td>
<p> If this is positive then it is taken as the known scale parameter. Negative signals that the 
scale paraemter is unknown. 0 signals that the scale parameter is 1  for Poisson and binomial and unknown otherwise. 
Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.    
</p>
</td></tr> 
<tr valign="top"><td><code>gamma</code></td>
<td>
<p>It is sometimes useful to inflate the model degrees of 
freedom in the GCV or UBRE/AIC score by a constant multiplier. This allows 
such a multiplier to be supplied. </p>
</td></tr> 
<tr valign="top"><td><code>knots</code></td>
<td>
<p>this is an optional list containing user specified knot values to be used for basis construction. 
For most bases the user simply supplies the knots to be used, which must match up with the <code>k</code> value
supplied (note that the number of knots is not always just <code>k</code>). 
See <code><a href="smooth.construct.tp.smooth.spec.html">tprs</a></code> for what happens in the <code>"tp"/"ts"</code> case. 
Different terms can use different numbers of knots, unless they share a covariate.
</p>
</td></tr>
<tr valign="top"><td><code>sp</code></td>
<td>
<p>A vector of smoothing parameters can be provided here.
Smoothing parameters must be supplied in the order that the smooth terms appear in the model 
formula. Negative elements indicate that the parameter should be estimated, and hence a mixture 
of fixed and estimated parameters is possible. If smooths share smoothing parameters then <code>length(sp)</code> 
must correspond to the number of underlying smoothing parameters.</p>
</td></tr>
<tr valign="top"><td><code>min.sp</code></td>
<td>
<p>Lower bounds can be supplied for the smoothing parameters. Note
that if this option is used then the smoothing parameters <code>full.sp</code>, in the 
returned object, will need to be added to what is supplied here to get the 
smoothing parameters actually multiplying the penalties. <code>length(min.sp)</code> should 
always be the same as the total number of penalties (so it may be longer than <code>sp</code>,
if smooths share smoothing parameters).</p>
</td></tr>
<tr valign="top"><td><code>paraPen</code></td>
<td>
<p>optional list specifying any penalties to be applied to parametric model terms. 
<code><a href="gam.models.html">gam.models</a></code> explains more.</p>
</td></tr>
<tr valign="top"><td><code>chunk.size</code></td>
<td>
<p>The model matrix is created in chunks of this size, rather than ever being formed whole. 
Reset to <code>4*p</code> if <code>chunk.size &lt; 4*p</code> where <code>p</code> is the number of coefficients.</p>
</td></tr>
<tr valign="top"><td><code>rho</code></td>
<td>
<p>An AR1 error model can be used for the residuals (based on dataframe order), of Gaussian-identity 
link models. This is the AR1 correlation parameter. Standardized residuals (approximately 
uncorrelated under correct model) returned in 
<code>std.rsd</code> if non zero. </p>
</td></tr>
<tr valign="top"><td><code>AR.start</code></td>
<td>
<p>logical variable of same length as data, <code>TRUE</code> at first observation of an independent
section of AR1 correlation. Very first observation in data frame does not need this. If <code>NULL</code> then 
there are no breaks in AR1 correlaion.</p>
</td></tr>
<tr valign="top"><td><code>discrete</code></td>
<td>
<p>with <code>method="fREML"</code> it is possible to discretize covariates for storage and efficiency reasons.
If <code>discrete</code> is <code>TRUE</code>, a number or a vector of numbers for each smoother term, then discretization happens. If numbers are supplied they give the number of discretization bins. Experimental at present. </p>
</td></tr>
<tr valign="top"><td><code>sparse</code></td>
<td>
<p>Deprecated. If all smooths are P-splines and all tensor products are of the form <code>te(...,bs="ps",np=FALSE)</code> 
then in principle computation could be made faster using sparse matrix methods, and you could set this to <code>TRUE</code>. 
In practice the speed up is disappointing, and the computation is less well conditioned than the default. See details.</p>
</td></tr>
<tr valign="top"><td><code>cluster</code></td>
<td>
<p><code>bam</code> can compute the computationally dominant QR decomposition in parallel using parLapply
from the <code>parallel</code> package, if it is supplied with a cluster on which to do this (a cluster here can be some cores of a 
single machine). See details and example code. 
</p>
</td></tr>
<tr valign="top"><td><code>nthreads</code></td>
<td>
<p>Number of threads to use for non-cluster computation (e.g. combining results from cluster nodes).
if <code>NA</code> set to <code>max(1,length(cluster))</code>.</p>
</td></tr>
<tr valign="top"><td><code>gc.level</code></td>
<td>
<p>to keep the memory footprint down, it helps to call the garbage collector often, but this takes 
a substatial amount of time. Setting this to zero means that garbage collection only happens when R decides it should. Setting to 2 gives frequent garbage collection. 1 is in between.</p>
</td></tr>
<tr valign="top"><td><code>use.chol</code></td>
<td>
<p>By default <code>bam</code> uses a very stable QR update approach to obtaining the QR decomposition
of the model matrix. For well conditioned models an alternative accumulates the crossproduct of the model matrix
and then finds its Choleski decomposition, at the end. This is somewhat more efficient, computationally.</p>
</td></tr>
<tr valign="top"><td><code>samfrac</code></td>
<td>
<p>For very large sample size Generalized additive models the number of iterations needed for the model fit can 
be reduced by first fitting a model to a random sample of the data, and using the results to supply starting values. This initial fit is run with sloppy convergence tolerances, so is typically very low cost. <code>samfrac</code> is the sampling fraction to use. 0.1 is often reasonable. </p>
</td></tr>
<tr valign="top"><td><code>drop.unused.levels</code></td>
<td>
<p>by default unused levels are dropped from factors before fitting. For some smooths 
involving factor variables you might want to turn this off. Only do so if you know what you are doing.</p>
</td></tr>
<tr valign="top"><td><code>G</code></td>
<td>
<p>if not <code>NULL</code> then this should be the object returned by a previous call to <code>bam</code> with 
<code>fit=FALSE</code>. Causes all other arguments to be ignored except <code>chunk.size</code>, <code>gamma</code>,<code>nthreads</code>, <code>cluster</code>, <code>rho</code>, <code>gc.level</code>, <code>samfrac</code>, <code>use.chol</code> and <code>method</code>.</p>
</td></tr>
<tr valign="top"><td><code>fit</code></td>
<td>
<p>if <code>FALSE</code> then the model is set up for fitting but not estimated, and an object is returned, suitable for passing as the <code>G</code> argument to <code>bam</code>.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>further arguments for 
passing on e.g. to <code>gam.fit</code> (such as <code>mustart</code>). </p>
</td></tr>
</tbody></table>


<h3>Details</h3>

<p> When <code>discrete=FALSE</code>, <code>bam</code> operates by first setting up the basis characteristics for the smooths, using a representative subsample 
of the data. Then the model matrix is constructed in blocks using <code><a href="predict.gam.html">predict.gam</a></code>. For each block the factor R,
from the QR decomposition of the whole model matrix is updated, along with Q'y. and the sum of squares of y. At the end of 
block processing, fitting takes place, without the need to ever form the whole model matrix. 
</p>
<p>In the generalized case, the same trick is used with the weighted model matrix and weighted pseudodata, at each step of the PIRLS.
Smoothness selection is performed on the working model at each stage (performance oriented iteration), to maintain the 
small memory footprint. This is trivial to justify in the case of GCV or Cp/UBRE/AIC based model selection, and 
for REML/ML is justified via the asymptotic multivariate normality of Q'z where z is the IRLS pseudodata. 
</p>
<p>For full method details see Wood, Goude and Shaw (2015).
</p>
<p>Note that POI is not as stable as the default nested iteration used with <code><a href="gam.html">gam</a></code>, but that for very large, information rich,
datasets, this is unlikely to matter much. 
</p>
<p>Note also that it is possible to spend most of the computational time on basis evaluation, if an expensive basis is used. In practice this means that the default <code>"tp"</code> basis should be avoided: almost any other basis (e.g. <code>"cr"</code> or <code>"ps"</code>) 
can be used in the 1D case, and tensor product smooths (<code>te</code>) are typically much less costly in the multi-dimensional case. 
</p>
<p>If <code>cluster</code> is provided as a cluster set up using <code><a href="../../parallel/html/makeCluster.html">makeCluster</a></code> (or <code>makeForkCluster</code>) from the <code>parallel</code> package, then the rate limiting QR decomposition of the model matrix is performed in parallel using this cluster. Note that the speed ups are often not that great. On a multi-core machine it is usually best to set the cluster size to the number of physical cores, which is often less than what is reported by <code><a href="../../parallel/html/detectCores.html">detectCores</a></code>. Using more than the number of physical cores can result in no speed up at all (or even a slow down). Note that a highly parallel BLAS may negate all advantage from using a cluster of cores. Computing in parallel of course requires more memory than computing in series. See examples.
</p>
<p>When <code>discrete=TRUE</code> the covariate data are first discretized. Discretization takes place on a smooth by smooth basis, or in the case of tensor product smooths (or any smooth that can be represented as such, such as random effects), separately for each marginal smooth. The required spline bases are then evaluated at the discrete values, and stored, along with index vectors indicating which original observation they relate to. Fitting is by a version of performance oriented iteration/PQL using REML smoothing parameter selection on each iterative working model (as for the default method). The iteration is based on the derivatives of the REML score, without computing the score itself, allowing the expensive computations to be reduced to one parallel block Cholesky decomposition per iteration (plus two basic operations of equal cost, but easily parallelized). Unlike standard POI/PQL, only one step of the smoothing parameter update for the working model is taken at each step (rather than iterating to the optimal set of smoothing parameters for each working model). At each step a weighted model matrix crossproduct of the model matrix is required - this is efficiently computed from the pre-computed basis functions evaluated at the discretized covariate values. Efficient computation with tensor product terms means that some terms within a tensor product may be re-ordered for maximum efficiency. Parallel computation is controlled using the <code>nthreads</code> argument. For this method no cluster computation is used, and the <code>parallel</code> package is not required. 
</p>
<p>If the deprecated argument <code>sparse=TRUE</code> then QR updating is replaced by an alternative scheme, in which the model matrix is stored whole
as a sparse matrix. This only makes sense if all smooths are P-splines and all tensor products are of the 
form <code>te(...,bs="ps",np=FALSE)</code>, but no check is made. The computations are then based on the Choleski decomposition of 
the crossproduct of the sparse model matrix. Although this crossproduct is nearly dense, sparsity should make its 
formation efficient, which is useful as it is the leading order term in the operations count. However there is no benefit 
in using sparse methods to form the Choleski decomposition, given that the crossproduct is dense. 
In practice the sparse matrix handling overheads mean that modest or no speed ups are produced 
by this approach, while the computation is less stable than the default, and the memory footprint often higher 
(but please let the author know if you find an example where the speedup is really worthwhile). 
</p>


<h3>Value</h3>

 
<p>An object of class <code>"gam"</code> as described in <code><a href="gamObject.html">gamObject</a></code>.
</p>


<h3>WARNINGS </h3>

<p>The routine will be slow if the default <code>"tp"</code> basis is used. 
</p>
<p>You must have more unique combinations of covariates than the model has total
parameters. (Total parameters is sum of basis dimensions plus sum of non-spline 
terms less the number of spline terms). 
</p>
<p>This routine is less stable than ‘gam’ for the same dataset.
</p>
<p>The negbin family is only supported for the *known theta* case.
</p>
<p>AIC computation does not currently take account of an AR1 model, if used.
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood simon.wood@r-project.org
</p>


<h3>References</h3>

<p>Wood, S.N., Goude, Y. &amp; Shaw S. (2015) Generalized additive models for large datasets. Journal of the Royal Statistical Society, Series C 64(1): 139-155.  
</p>


<h3>See Also</h3>

<p><code><a href="mgcv-parallel.html">mgcv.parallel</a></code>, 
<code><a href="mgcv-package.html">mgcv-package</a></code>, <code><a href="gamObject.html">gamObject</a></code>, <code><a href="gam.models.html">gam.models</a></code>, <code><a href="smooth.terms.html">smooth.terms</a></code>,
<code><a href="linear.functional.terms.html">linear.functional.terms</a></code>, <code><a href="s.html">s</a></code>,
<code><a href="te.html">te</a></code> <code><a href="predict.gam.html">predict.gam</a></code>,
<code><a href="plot.gam.html">plot.gam</a></code>, <code><a href="summary.gam.html">summary.gam</a></code>, <code><a href="gam.side.html">gam.side</a></code>,
<code><a href="gam.selection.html">gam.selection</a></code>, <code><a href="gam.control.html">gam.control</a></code>
<code><a href="gam.check.html">gam.check</a></code>, <code><a href="linear.functional.terms.html">linear.functional.terms</a></code> <code><a href="negbin.html">negbin</a></code>, <code><a href="magic.html">magic</a></code>,<code><a href="vis.gam.html">vis.gam</a></code>
</p>


<h3>Examples</h3>

<pre>library(mgcv)
## See help("mgcv-parallel") for using bam in parallel

## Some examples are marked 'Not run' purely to keep 
## checking load on CRAN down. Sample sizes are small for 
## the same reason.

set.seed(3)
dat &lt;- gamSim(1,n=25000,dist="normal",scale=20)
bs &lt;- "cr";k &lt;- 12
b &lt;- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)+
           s(x3,bs=bs),data=dat)
summary(b)
plot(b,pages=1,rug=FALSE)  ## plot smooths, but not rug
plot(b,pages=1,rug=FALSE,seWithMean=TRUE) ## `with intercept' CIs

## Not run:  
ba &lt;- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
            s(x3,bs=bs,k=k),data=dat,method="GCV.Cp") ## use GCV
summary(ba)
## End(Not run)

## A Poisson example...

k &lt;- 15
dat &lt;- gamSim(1,n=21000,dist="poisson",scale=.1)

system.time(b1 &lt;- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
            data=dat,family=poisson()))
b1


## Sparse smoother example (deprecated)...
## Not run: 
dat &lt;- gamSim(1,n=10000,dist="poisson",scale=.1)
system.time( b3 &lt;- bam(y ~ te(x0,x1,bs="ps",k=10,np=FALSE)+
             s(x2,bs="ps",k=30)+s(x3,bs="ps",k=30),data=dat,
             method="REML",family=poisson(),sparse=TRUE))
b3
## End(Not run)

</pre>

<hr><div style="text-align: center;">[Package <em>mgcv</em> version 1.8-12 <a href="00Index.html">Index</a>]</div>

<!-- Mirrored from stat.ethz.ch/R-manual/R-patched/RHOME/library/mgcv/html/bam.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 04 May 2016 17:01:41 GMT -->

</body></html>