<html><!-- Mirrored from stat.ethz.ch/R-manual/R-patched/RHOME/library/stats/html/ks.test.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 04 May 2016 17:02:03 GMT --><!-- Added by HTTrack --><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><!-- /Added by HTTrack -->
<title>Kolmogorov-Smirnov Tests</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">
</head><body>

<table width="100%" summary="page for ks.test {stats}"><tbody><tr><td>ks.test {stats}</td><td style="text-align: right;">R Documentation</td></tr></tbody></table>

<h2>Kolmogorov-Smirnov Tests</h2>

<h3>Description</h3>

<p>Perform a one- or two-sample Kolmogorov-Smirnov test.
</p>


<h3>Usage</h3>

<pre>ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tbody><tr valign="top"><td><code>x</code></td>
<td>
<p>a numeric vector of data values.</p>
</td></tr>
<tr valign="top"><td><code>y</code></td>
<td>
<p>either a numeric vector of data values, or a character string
naming a cumulative distribution function or an actual cumulative
distribution function such as <code>pnorm</code>.  Only continuous CDFs
are valid.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>parameters of the distribution specified (as a character
string) by <code>y</code>.</p>
</td></tr>
<tr valign="top"><td><code>alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code> (default), <code>"less"</code>, or
<code>"greater"</code>.  You can specify just the initial letter of the
value, but the argument name must be give in full.
See ‘Details’ for the meanings of the possible values.</p>
</td></tr>
<tr valign="top"><td><code>exact</code></td>
<td>
<p><code>NULL</code> or a logical indicating whether an exact
p-value should be computed.  See ‘Details’ for the meaning of
<code>NULL</code>.  Not available in the two-sample case for a one-sided
test or if ties are present.</p>
</td></tr>
</tbody></table>


<h3>Details</h3>

<p>If <code>y</code> is numeric, a two-sample test of the null hypothesis
that <code>x</code> and <code>y</code> were drawn from the same <em>continuous</em>
distribution is performed.
</p>
<p>Alternatively, <code>y</code> can be a character string naming a continuous
(cumulative) distribution function, or such a function.  In this case,
a one-sample test is carried out of the null that the distribution
function which generated <code>x</code> is distribution <code>y</code> with
parameters specified by <code>...</code>.
</p>
<p>The presence of ties always generates a warning, since continuous
distributions do not generate them.  If the ties arose from rounding
the tests may be approximately valid, but even modest amounts of
rounding can have a significant effect on the calculated statistic.
</p>
<p>Missing values are silently omitted from <code>x</code> and (in the
two-sample case) <code>y</code>.
</p>
<p>The possible values <code>"two.sided"</code>, <code>"less"</code> and
<code>"greater"</code> of <code>alternative</code> specify the null hypothesis
that the true distribution function of <code>x</code> is equal to, not less
than or not greater than the hypothesized distribution function
(one-sample case) or the distribution function of <code>y</code> (two-sample
case), respectively.  This is a comparison of cumulative distribution
functions, and the test statistic is the maximum difference in value,
with the statistic in the <code>"greater"</code> alternative being
<i>D^+ = max[F_x(u) - F_y(u)]</i>.
Thus in the two-sample case <code>alternative = "greater"</code> includes
distributions for which <code>x</code> is stochastically <em>smaller</em> than
<code>y</code> (the CDF of <code>x</code> lies above and hence to the left of that
for <code>y</code>), in contrast to <code><a href="t.test.html">t.test</a></code> or
<code><a href="wilcox.test.html">wilcox.test</a></code>.
</p>
<p>Exact p-values are not available for the two-sample case if one-sided
or in the presence of ties.  If <code>exact = NULL</code> (the default), an
exact p-value is computed if the sample size is less than 100 in the
one-sample case <em>and there are no ties</em>, and if the product of
the sample sizes is less than 10000 in the two-sample case.
Otherwise, asymptotic distributions are used whose approximations may
be inaccurate in small samples.  In the one-sample two-sided case,
exact p-values are obtained as described in Marsaglia, Tsang &amp; Wang
(2003) (but not using the optional approximation in the right tail, so
this can be slow for small p-values).  The formula of Birnbaum &amp;
Tingey (1951) is used for the one-sample one-sided case.
</p>
<p>If a single-sample test is used, the parameters specified in
<code>...</code> must be pre-specified and not estimated from the data.
There is some more refined distribution theory for the KS test with
estimated parameters (see Durbin, 1973), but that is not implemented
in <code>ks.test</code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table summary="R valueblock">
<tbody><tr valign="top"><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr valign="top"><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr valign="top"><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>a character string indicating what type of test was
performed.</p>
</td></tr>
<tr valign="top"><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</tbody></table>


<h3>Source</h3>

<p>The two-sided one-sample distribution comes <em>via</em>
Marsaglia, Tsang and Wang (2003).
</p>


<h3>References</h3>

<p>Z. W. Birnbaum and Fred H. Tingey (1951),
One-sided confidence contours for probability distribution functions.
<em>The Annals of Mathematical Statistics</em>, <b>22</b>/4, 592–596.
</p>
<p>William J. Conover (1971),
<em>Practical Nonparametric Statistics</em>.
New York: John Wiley &amp; Sons.
Pages 295–301 (one-sample Kolmogorov test),
309–314 (two-sample Smirnov test).
</p>
<p>Durbin, J. (1973),
<em>Distribution theory for tests based on the sample distribution
function</em>.  SIAM.
</p>
<p>George Marsaglia, Wai Wan Tsang and Jingbo Wang (2003),
Evaluating Kolmogorov's distribution.
<em>Journal of Statistical Software</em>, <b>8</b>/18.
http://www.jstatsoft.org/v08/i18/.
</p>


<h3>See Also</h3>

<p><code><a href="shapiro.test.html">shapiro.test</a></code> which performs the Shapiro-Wilk test for
normality.
</p>


<h3>Examples</h3>

<pre>require(graphics)

x &lt;- rnorm(50)
y &lt;- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 &lt;- rnorm(50, -1)
plot(ecdf(x), xlim = range(c(x, x2)))
plot(ecdf(x2), add = TRUE, lty = "dashed")
t.test(x, x2, alternative = "g")
wilcox.test(x, x2, alternative = "g")
ks.test(x, x2, alternative = "l")
</pre>

<hr><div style="text-align: center;">[Package <em>stats</em> version 3.2.2 <a href="00Index.html">Index</a>]</div>

<!-- Mirrored from stat.ethz.ch/R-manual/R-patched/RHOME/library/stats/html/ks.test.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 04 May 2016 17:02:03 GMT -->

</body></html>